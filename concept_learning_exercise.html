<!DOCTYPE html>
<html lang="en">

  <head>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <title>AIMA Exercises</title>


    <!-- Custom fonts -->
    <link href='https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>
    

    <!-- Custom styles -->
    <link href="css/clean-blog.min.css" rel="stylesheet">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css">

    <!-- jQuery library -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>

    <!-- Latest compiled JavaScript -->
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>
    <script type="text/x-mathjax-config">
            MathJax.Hub.Config({
            tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
                              });
    </script>
    <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>

    
    

  </head>

  <body>
      
    <!-- Navigation -->
      <nav class="navbar navbar-expand-lg navbar-light bg-light navbar-fixed-top">
  <a class="navbar-brand" href="index.html">AIMA</a>
  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  <div class="collapse navbar-collapse" id="navbarSupportedContent">
    <ul class="navbar-nav mr-auto">
      
      <li class="nav-item dropdown">
        <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
          <b>Exercises</b>
        </a>
        <div class="dropdown-menu" aria-labelledby="navbarDropdown">
                <a href="intro_exercise.html" class="dropdown-item">✓ Intro Exercise</a>
			    <a href="agents_exercise.html" class="dropdown-item">Agents Exercise</a>
			    <a href="search_exercise.html" class="dropdown-item">Search Exercise</a>
			    <a href="advanced_search_exercise.html" class="dropdown-item">✓ Advanced Search Exercise</a>
			    <a href="game_playing_exercise.html" class="dropdown-item">Game Playing Exercise</a>
			    <a href="csp_exercise.html" class="dropdown-item">Csp Exercise</a>
			    <a href="knowledge_logic_exercise.html" class="dropdown-item">Knowledge Logic Exercise</a>
			    <a href="fol_exercise.html" class="dropdown-item">Fol Exercise</a>
			    <a href="logical_inference_exercise.html" class="dropdown-item">Logical Inference Exercise</a>
			    <a href="planning_exercise.html" class="dropdown-item">Planning Exercise</a>
			    <a href="advanced_planning_exercise.html" class="dropdown-item">✓ Advanced Planning Exercise</a>
			    <a href="kr_exercise.html" class="dropdown-item">Kr Exercise</a>
			    <a href="probability_exercise.html" class="dropdown-item">Probability Exercise</a>
			    <a href="bayes_nets_exercise.html" class="dropdown-item">Bayes Nets Exercise</a>
			    <a href="dbn_exercise.html" class="dropdown-item">Dbn Exercise</a>
			    <a href="decision_theory_exercise.html" class="dropdown-item">Decision Theory Exercise</a>
			    <a href="complex_decisions_exercise.html" class="dropdown-item">Complex Decisions Exercise</a>
			    <a href="concept_learning_exercise.html" class="dropdown-item active">Concept Learning Exercise</a>
			    <a href="ilp_exercise.html" class="dropdown-item">Ilp Exercise</a>
		            <a href="bayesian_learning_exercise.html" class="dropdown-item">Bayesian Learning Exercise</a>
			    <a href="reinforcement_learning_exercise.html" class="dropdown-item">Reinforcement Learning Exercise</a>
			    <a href="nlp_communicating_exercise.html" class="dropdown-item">Nlp Communicating Exercise</a>
			    <a href="nlp_english_exercise.html" class="dropdown-item">Nlp English Exercise</a>
			    <a href="perception_exercise.html" class="dropdown-item">Perception Exercise</a>
			    <a href="robotics_exercise.html" class="dropdown-item">Robotics Exercise</a>
			    <a href="philosophy_exercise.html" class="dropdown-item">Philosophy Exercise</a>
          <div class="dropdown-divider"></div>
          <a href="future_exercise.html" class="dropdown-item" >Future Exercise</a>
        </div>
      </li>
    </ul>
  </div>
</nav>

<br />

    <!-- Page Header -->
<header><br /><br /><br /></header>
      
      
      

    <!-- Main Content -->
    <div class="container">
	    <div class="letter">

      <div class="row">
        <div class="col-lg-8 col-md-10 mx-auto">
          <div class="post-preview">
            <h5><b>EXERCISES</b></h1>
        <h1 id="18learningfromexamples">18. Learning from Examples</h1>

<p><strong>18.17</strong> Prove that a decision list can represent the same function as a decision
tree while using at most as many rules as there are leaves in the
decision tree for that function. Give an example of a function
represented by a decision list using strictly fewer rules than the
number of leaves in a minimal-sized decision tree for that same
function.</p>
<p><strong>18.18</strong> [DL-expressivity-exercise]This exercise concerns the expressiveness of
decision lists (Section <a href="#/">learning-theory-section</a>).</p>
<ol>
<li><p>Show that decision lists can represent any Boolean function, if the
size of the tests is not limited.</p>
</li>
<li><p>Show that if the tests can contain at most $k$ literals each, then
decision lists can represent any function that can be represented by
a decision tree of depth $k$.</p>
</li>
</ol>
<p><strong>18.19</strong> [knn-mean-mode] Suppose a $7$-nearest-neighbors regression search
returns $ \{7, 6, 8, 4, 7, 11, 100\} $ as the 7 nearest $y$ values for a
given $x$ value. What is the value of $\hat{y}$ that minimizes the $L_1$
loss function on this data? There is a common name in statistics for
this value as a function of the $y$ values; what is it? Answer the same
two questions for the $L_2$ loss function.</p>
<p><strong>18.20</strong> [knn-mean-mode] Suppose a $7$-nearest-neighbors regression search
returns $ \{4, 2, 8, 4, 9, 11, 100\} $ as the 7 nearest $y$ values for a
given $x$ value. What is the value of $\hat{y}$ that minimizes the $L_1$
loss function on this data? There is a common name in statistics for
this value as a function of the $y$ values; what is it? Answer the same
two questions for the $L_2$ loss function.</p>
<p><strong>18.21</strong> [svm-ellipse-exercise] Figure <a href="#/">kernel-machine-figure</a>
showed how a circle at the origin can be linearly separated by mapping
from the features $(x_1, x_2)$ to the two dimensions $(x_1^2, x_2^2)$.
But what if the circle is not located at the origin? What if it is an
ellipse, not a circle? The general equation for a circle (and hence the
decision boundary) is $(x_1-a)^2 +
(x_2-b)^2 - r^2{{\,{=}\,}}0$, and the general equation for an ellipse is
$c(x_1-a)^2 + d(x_2-b)^2 - 1 {{\,{=}\,}}0$.</p>
<ol>
<li><p>Expand out the equation for the circle and show what the weights
$w_i$ would be for the decision boundary in the four-dimensional
feature space $(x_1, x_2, x_1^2, x_2^2)$. Explain why this means
that any circle is linearly separable in this space.</p>
</li>
<li><p>Do the same for ellipses in the five-dimensional feature space
$(x_1, x_2, x_1^2, x_2^2, x_1 x_2)$.</p>
</li>
</ol>
<p><strong>18.22</strong> [svm-exercise] Construct a support vector machine that computes the
xor function. Use values of +1 and –1 (instead of 1 and 0)
for both inputs and outputs, so that an example looks like $([-1, 1],
1)$ or $([-1, -1], -1)$. Map the input $[x_1,x_2]$ into a space
consisting of $x_1$ and $x_1\,x_2$. Draw the four input points in this
space, and the maximal margin separator. What is the margin? Now draw
the separating line back in the original Euclidean input space.</p>
<p><strong>18.23</strong> [ensemble-error-exercise] Consider an ensemble learning algorithm that
uses simple majority voting among $K$ learned hypotheses.
Suppose that each hypothesis has error $\epsilon$ and that the errors
made by each hypothesis are independent of the others’. Calculate a
formula for the error of the ensemble algorithm in terms of $K$
and $\epsilon$, and evaluate it for the cases where
$K{{\,{=}\,}}5$, 10, and 20 and $\epsilon{{\,{=}\,}}{0.1}$, 0.2,
and 0.4. If the independence assumption is removed, is it possible for
the ensemble error to be <em>worse</em> than $\epsilon$?</p>
<p><strong>18.24</strong> Construct by hand a neural network that computes the xor
function of two inputs. Make sure to specify what sort of units you are
using.</p>
<p><strong>18.25</strong> A simple perceptron cannot represent xor (or, generally,
the parity function of its inputs). Describe what happens to the weights
of a four-input, hard-threshold perceptron, beginning with all weights
set to 0.1, as examples of the parity function arrive.</p>
<p><strong>18.26</strong> [linear-separability-exercise] Recall from
Chapter <a href="#/">concept-learning-chapter</a> that there are
$2^{2^{{n}}}$ distinct Boolean functions of ${{n}}$ inputs. How many of
these are representable by a threshold perceptron?</p>
<p><strong>18.27</strong> Consider the following set of examples, each with six inputs and one
target output:</p>
<table>
<thead><tr>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>$\textbf{x}_1$</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>$\textbf{x}_2$</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>$\textbf{x}_3$</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>$\textbf{x}_4$</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>$\textbf{x}_5$</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>$\textbf{x}_6$</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>$\textbf{T}$</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
</tbody>
</table>
<ol>
<li><p>Run the perceptron learning rule on these data and show the
final weights.</p>
</li>
<li><p>Run the decision tree learning rule, and show the resulting
decision tree.</p>
</li>
<li><p>Comment on your results.</p>
</li>
</ol>
<p><strong>18.28</strong> [perceptron-ML-gradient-exercise]
Section <a href="#/">logistic-regression-section</a>
(page <a href="#/">logistic-regression-section</a>) noted that the output of the logistic function
could be interpreted as a <em>probability</em> $p$ assigned by the
model to the proposition that $f(\textbf{x}){{\,{=}\,}}1$; the probability that
$f(\textbf{x}){{\,{=}\,}}0$ is therefore $1-p$. Write down the probability $p$
as a function of $\textbf{x}$ and calculate the derivative of $\log p$ with
respect to each weight $w_i$. Repeat the process for $\log (1-p)$. These
calculations give a learning rule for minimizing the
negative-log-likelihood loss function for a probabilistic hypothesis.
Comment on any resemblance to other learning rules in the chapter.</p>
<p><strong>18.29</strong> [linear-nn-exercise]Suppose you had a neural network with linear
activation functions. That is, for each unit the output is some constant
$c$ times the weighted sum of the inputs.</p>
<ol>
<li><p>Assume that the network has one hidden layer. For a given assignment
to the weights $\textbf{w}$, write down equations for the value of the
units in the output layer as a function of $\textbf{w}$ and the input layer
$\textbf{x}$, without any explicit mention of the output of the
hidden layer. Show that there is a network with no hidden units that
computes the same function.</p>
</li>
<li><p>Repeat the calculation in part (a), but this time do it for a
network with any number of hidden layers.</p>
</li>
<li><p>Suppose a network with one hidden layer and linear activation
functions has $n$ input and output nodes and $h$ hidden nodes. What
effect does the transformation in part (a) to a network with no
hidden layers have on the total number of weights? Discuss in
particular the case $h \ll n$.</p>
</li>
</ol>
<p><strong>18.30</strong> Implement a data structure for layered, feed-forward neural networks,
remembering to provide the information needed for both forward
evaluation and backward propagation. Using this data structure, write a
function NEURAL-NETWORK-OUTPUT that takes an example and a network and computes the
appropriate output values.</p>
<p><strong>18.31</strong> Suppose that a training set contains only a single example, repeated 100
times. In 80 of the 100 cases, the single output value is 1; in the
other 20, it is 0. What will a back-propagation network predict for this
example, assuming that it has been trained and reaches a global optimum?
(<em>Hint:</em> to find the global optimum, differentiate the
error function and set it to zero.)</p>
<p><strong>18.32</strong> The neural network whose learning performance is measured in
Figure <a href="#/">restaurant-back-prop-figure</a> has four hidden
nodes. This number was chosen somewhat arbitrarily. Use a
cross-validation method to find the best number of hidden nodes.</p>
<p><strong>18.33</strong> [embedding-separability-exercise] Consider the problem of separating
$N$ data points into positive and negative examples using a linear
separator. Clearly, this can always be done for $N{{\,{=}\,}}2$ points
on a line of dimension $d{{\,{=}\,}}1$, regardless of how the points are
labeled or where they are located (unless the points are in the same
place).</p>
<ol>
<li><p>Show that it can always be done for $N{{\,{=}\,}}3$ points on a
plane of dimension $d{{\,{=}\,}}2$, unless they are collinear.</p>
</li>
<li><p>Show that it cannot always be done for $N{{\,{=}\,}}4$ points on a
plane of dimension $d{{\,{=}\,}}2$.</p>
</li>
<li><p>Show that it can always be done for $N{{\,{=}\,}}4$ points in a
space of dimension $d{{\,{=}\,}}3$, unless they are coplanar.</p>
</li>
<li><p>Show that it cannot always be done for $N{{\,{=}\,}}5$ points in a
space of dimension $d{{\,{=}\,}}3$.</p>
</li>
<li><p>The ambitious student may wish to prove that $N$ points in general
position (but not $N+1$) are linearly separable in a space of
dimension $N-1$.</p>
</li>
</ol>
		</div>
	      </div>
      </div>
    </div>

    <hr>

    <!-- Footer -->
    <footer>
      <div class="container" >
        <div class="row">
          <div class="col-lg-8 col-md-10 mx-auto">
            <ul class="list-inline text-center">
              
        
              <li class="list-inline-item">
                <a href="https://github.com/aimacode/aima-exercises">
                  <span class="fa-stack fa-lg">
                    <i class="fa fa-circle fa-stack-2x"></i>
                    <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
                
                
            </ul>
            <p class="copyright text-muted">Copyright &copy; Peter Norvig</p>
          </div>
        </div>
      </div>
    </footer>


  </body>

</html>
